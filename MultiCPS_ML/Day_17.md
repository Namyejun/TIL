## **분류: KNN(K-nearest Neighbor Classifier)**

### KNN 알고리즘

- 가장 간단한 지도학습 머신러닝 알고리즘
- 훈련데이터를 저장해 두는 것이 모델을 만드는 과정의 전부임
- 새로운 데이터가 입력되면 그 새로운 데이터 주변의 가장 가까운 K개의 훈련 데이터의 ;레이블을 확인한 뒤, 가장 많이 보이는 라벨로 분류하는 방법.
    - K = 3인 경우 새로운 데이터 ⭐에 대한 예측 : 남자
        
        ![Untitled](img/Untitled%2020.png)
        

### K의 결정

- KNN에서 K의 결정은 매우 중요한 문제임.
- K가 작으면 이상점 등의 노이즈에 민감하게 반응하는 과적합의 문제
- K가 크면 자료의 패턴을 잘 파악할 수 없어 예측 성능이 저하됨.
- 검증용(validation) 데이터를 이용하여 주어진 훈련 데이터에 가장 적절한 K를 찾아야함.
    - cross validation 사용

### 거리의 측정

- n개의 특성변수를 가지는 자료에서 두 개의 관찰점
$a = (a_1, a_2, …, a_n)$와 $b = (b_1, b_2, …, b_n)$ 간의 거리를 측정하는 문제
- 유클리디안 거리
$d(a, b) = \sqrt {\sum_{i=1}^n (a_i - b_i)^2}$
모든 특성변수에 동일한 상수를 곱한 경우, 관찰치들 간의 **유클리드 거리의 순위**는 바뀌지 않는다. → 이게 뭔말? 아마도 존재하는 모든 데이터의 모든 특성변수에다가 스케일링을 한다는것같음
- 맨해튼 거리
$d(a, b) = \sum_{i=1}^n |a_i - b_i|$
- 민코우스키 거리$d(a, b) = (\sum_{i=1}^n |a_i - b_i|^p)^{1/p}$
- 자료의 스케일에 차이가 있는 경우,
스케일이 큰 특성변수에 의해 거리가 결정되어 버릴 수 있음.
따라서 각 특성변수 별로 스케일이 유사해지도록 표준화 변환(Z score) 또는 min-max 변환으로 스케일링을 해준 뒤 거리를 재는 것이 적절함.
