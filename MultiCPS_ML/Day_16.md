## **분류: 나이브베이즈(Naïve Bayes)**

### 나이브 베이즈 분류기 아이디어

- 목표변수 Y가 2개의 범주 $C_1, C_2$를 가진다고 할 때
특성변수 X의 값을 이용하여 Y의 범주를 예측하는 문제.
- X = x로 주어졌을 때 Y의 각 범주에 대한 조건부 확률을 비교하고자 함
- $P[C_1|x] \gt P[C_2|x]$면 $C_1$으로 분류하고, 그렇지 않으면 $C_2$로 분류함
- $P[C_k|x]$는 훈련 자료에서 추정하기 어려움 ⇒ 베이즈 정리를 이용
    - $P[C_k|x]$에 비해 $P[C_1], P[C_2], P[x|C_1], P[x|C_2]$는 측정이 가능함
- 나이브 베이즈 분류기는 생성(generative) 모델임.

### 베이즈 정리의 활용

- 베이즈 정리에 의하면
$P[C_k|x] = \dfrac {P[x \cap C_k]} {P[x]} = \dfrac {P[x|C_k]P[C_k]} {P[x]}$, k = 1, 2
- $P[C_1|x] \gt P[C_2|x]$인지 여부.
⇒$\dfrac {P[x|C_1]P[C_1]} {P[x]} \gt \dfrac {P[x|C_2]P[C_2]} {P[x]}$인지 여부.
⇒$P[x|C_1]P[C_1] > P[x|C_2]P[C_2]$인지 여부로 판단하고자 함.
- $P[x|C_k]$와 $P[C_k]$는 훈련 데이터를 이용하여 쉽게 추정할 수 있음

### n개의 특성변수를 가지는 분류 문제

- $P[C_k|x_1, ..., x_n]$에 베이즈 정리를 적용.
$P[C_k|x_1, ..., x_n] = \dfrac {P[C_k]P[x_1|C_k]P[x_2|x_1, C_k] …P[x_n|x_1, …, x_{n-1},C_k]} {P[x_1, …, x_n]}, \ \ k = 1,2$
- 각 특성변수들이 모두 독립이라고 가정하면,$P[C_k]P[x_1|C_k]P[x_2|x_1, C_k] …P[x_n|x_1, …, x_{n-1},C_k]$
$= P[C_k]P[x_1|C_k]P[x_2|C_k]…P[x_n|C_k]$
$=P[C_k](\displaystyle\prod_{i=1}^nP[x_i|C_k]), \ \  k = 1,2$
- 예측
$P[C_1](\displaystyle \prod_{i=1}^n P[x_i|C_1]) \ge P[C_2](\displaystyle \prod_{i=1}^n P[x_i|C_2])$면 범주 1로 분류
$P[C_1](\displaystyle \prod_{i=1}^n P[x_i|C_1]) \lt P[C_2](\displaystyle \prod_{i=1}^n P[x_i|C_2])$면 범주 2로 분류
    - $P[C_k]$
        - k번째 범주에 속할 확률
    - $P[x_i|C_k]$
        - 목표변수가 k번째 범주일 때, 각 특성변수 $x_i$가 관찰될 확률.
        - $x_i$의 자료형식(범주형/개수형/연속형)에 따라 적절한 확률분포를 가정하여 추정
            - 범주형을 바로 비율을 추정하면 됨, 이론적으로는 Multinoulli라는 분포를 가정하고 모수를 추정하는 셈.
            - 개수형인 경우에는 Multinomial 분포를 사용
            - 정규형이나 x의 분포에 맞게 적절한 확률 분포를 이용해서 추정한다.
    
    ![$x_i$의 자료형이 연속형인 경우](%5BProDS%5D%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%8B%E1%85%B5%E1%84%85%E1%85%A9%E1%86%AB%20%E1%84%86%E1%85%B5%E1%86%BE%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%200fd2bfc90cad4606942dfafe60f5a110/Untitled%2019.png)
    
    $x_i$의 자료형이 연속형인 경우
    

### 나이브 베이즈의 장단점

- 장점
    - 데이터의 크기가 커도 연산 속도가 빠름
    - 학습에 필요한 데이터 양이 적어도 좋은 성능을 보이는 편.
    - 다양한 텍스트 분류나 추천 등에 활용됨
- 단점
    - Zero frequency 문제나 Underflow 문제가 있음
        - Zero frequency는 특정 빈도가 0인 경우 확률 추정지 계산에 생기는 문제,
        Underflow 문제는 컴퓨터가 계산 가능한 수준보다 너무 작은 숫자들이 만들어지는 문제
    - 모든 독립변수가 독립이라는 가정이 너무 단순함.
        - 실제로는 연관된 경우가 훨씬 많음