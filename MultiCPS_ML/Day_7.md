## 특성 공학: 개요, 특성 추출(Feature Extraction) 방법론

### 특성 공학

- 특성 공간 방법론
    - 특성 선택 : 가지고 있는 특성 중 더 유용한 특성을 선택
    - 특성 추출 : 가지고 있는 특성을 결합하여 더 유용한 특성을 생성

- 주요 특성 추출법
    - PCA(Principal component analysis)
    - SVD(Singular Value Decomposition)
    - LDA(Linear discriminant analysis)
    - NMF(Non-negative matrix factorization)
    
    특성을 압축한다고 함
    

### 주성분 분석(PCA)

![Untitled](img/Untitled%205.png)

- 서로 연관되어 있는 변수들 ($x_1,...,x_k$)이 관찰되었을 때, 이 변수들이 전체적으로 가지고 있는 정보들을 최대한 확보하는 적은 수의 새로운 변수(주성분, PC)를 생성하는 방법
    - $x_1,...,x_k$가 서로 중복되는 정보를 많이 반영하고 있을 때 PCA가 특성추출을 잘할 수 있다.
    - 주성분을 k개 $y_1,...,y_k$개 만들 수 있는데 앞에 있을 수록 x들의 변동을 잘 설명한다.
        - ex) $y_1 = 0.5 \times x_1 + 0.5 \times  x_2$
        - 모든 점의 분산이 가장 커지는 방향으로 새로 축을 정한다.
        - 새로운 축을 정할 때는 y1이 가지지 못한 독립정보를 가지며 변동성이 가장 커지도록 정한다. → y1에 수직인 축
        - 위 그림에선 2차원이라 주성분이 2개 밖에 안나옴 y1과 y1에 수직인 축
    - 그래서 앞에서부터 몇개를 골라서 차원 축소가 가능하다.
    

### 주성분 분석의 목적

- 자료에서 변동이 큰 축을 탐색함
- 변수들에 담긴 정보의 손실을 최소화하면서 차원을 축소한다.
- 서로 상관이 없거나 독립적인 새로운 변수인 주성분을 통해 데이터의 해석을 용이하게 한다.

### 주성분 분석 아이디어

- k개의 특성변수 $x_1, ..., x_k$의 주성분이 $y_1, ..., y_k$라면 이들은 $x_1, ..., x_k$의 선형 결합식으로 아래와 같이 표현됨.
    
    $y_1 = l_{11}x_1 + ... + l_{k1}x_k$  → $V[y_1]$이 최대가 되도록 하는 것을 찾는다.
    
    $y_2 = l_{12}x_1 + ... + l_{k2}x_k$ → $y_1$에 직교하는 축이며 분산이 두 번째로 최대가 되는 것을 찾는다.
    
    …
    
    $y_k = l_{1k}x_1 + ... + l_{kk}x_k$
    
    1. $V[y_1]$가 최대가 되도록 하는 길이가 1인 벡터 $l_1 = (l_{11}, l_{21}, ...,l_{k1})$로 첫 번째 주성분 $y_1$을 결정.
    2. $y_1$에 직교하는 축($Cov[y_2, y_1] = 0$)이며, $V[y_2]$를 최대로 하는 길이가 1인 벡터 $l_2 = (l_{12}, l_{22}, ...,l_{k2})$로 첫 번째 주성분 $y_1$을 결정.
    3. j = k가 될 때까지 반복

### 주성분 분석에 관한 기하학적 의미

- 주성분 축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석할 수 있다.
    - 첫 번째 주성분 축은 데이터의 변동이 가장 커지는 축임.
    - 두 번째 주성분 축은 첫 번째 주성분 축과 직교하며 첫 번째 주성분 축 다음으로 데이터의 변동이 큰 축을 나타냄
    - 각 관찰치 별 주성분 점수는 대응하는 원 자료 값들의 주성분 좌표축에서의 좌표 값에 해당함. → 주성분 좌표 축에 정사영했을 때 그 값
    - 자료들의 공분산 행렬이 대각행렬이 되도록 회전한 것으로 해석. → 나머지 축과의 공분산이 0이 되도록 골랐으니 대각행렬을 제외하고는 다 0
    - 주성분 축은 원래 변수들의 좌표축이 직교 회전 변환된 것으로 해석할 수 있음
    

### 특성값 분해

- 특이값 분해 : 임의의 $n \times d$ 행렬 A는 $A = U \Sigma V^T$로 분해가 가능함. → 수치해석 때 배움
    - U와 V는 직교행렬 : $U^TU = I_{n \times n}$, $V^TV = I_{d \times d}$
    - U의 각 열을 A의 왼쪽 특성벡터, V의 각 열을 A의 오른쪽 특성벡터라고 함.
    - $\Sigma$는 $n \times d$의 대각행렬 : 대각원소를 A의 특성값이라고 한다.
    

### 특이값 분해와 차원 축소

- $U$의 각 열을 $u_i$, $i =1, 2, …, n$
- $V^T$의 각 행을 $v_i^T$, $i = 1, 2, …, d$
- $\Sigma$의 0이 아닌 대각 원소를 $\lambda_i, i = 1,2, ..., r (\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_r)$

$A = U \Sigma V^T = \sqrt{\lambda_1}u_1v_1^T + \sqrt{\lambda_2}u_2v_2^T + ... + \sqrt{\lambda_m}u_mv_m^T + ... + \sqrt{\lambda_r}u_rv_r^T$

정보가 많은 순서대로 m개만 이용하여 근사하는 경우 **m계수 근사**라고 함 → A에 상당히 가까워진다. **Rank m 근사**, 이미지 압축에 많이 쓰임 →  커다란 행렬을 적은 데이터만으로도 근사할 수 있다.

$V^T$ → $A^TA$(A의 공분산 행렬)를 했을 때 고유벡터라는 것으로 컬럼들이 구성되는 행렬

### 주성분 분석과 특성값 분해의 관계

- A의 오른쪽 특성벡터($V^T$)는 A의 공분산 행렬($A^TA$)의 고유벡터와 동일함
- 자료 행렬(원 데이터)에 대한 특성값 분해로 주성분을 도출할 수 있다.
    
    → ($V^T$)의 컬럼 값들이 X들에 어떤 계수를 곱해서 더하는지 결정한다. $l_{11}, … l_{k1}$이거 얘기인가
    
